The article details the implementation of a Model Context Protocol (MCP) server using the Micronaut framework, addressing the current absence of a dedicated Micronaut module for MCP server development, unlike frameworks such as Quarkus or Spring Boot. Driven by his preference for Micronaut, author Guillaume Laforge embarked on building a quick implementation through a method he terms "vibe coding" with the assistance of Google's Gemini LLM.

**Background and Motivation:**
Laforge notes that he had previously explored implementing an MCP server in Java using the reference implementation, served as a servlet via Jetty, and called by LangChain4j's MCP support. However, for Micronaut, he aimed for a more "genuine and native" server implementation rather than integrating a servlet.

**Vibe Coding with Gemini:**
The core of the implementation process revolves around "vibe coding," a concept defined by Andrej Karpathy as interacting with an LLM to build a new prototype or weekend project by iterating with the LLM until it functions, without the developer directly touching or even looking at the generated code initially. This approach is distinct from using AI assistance for production-ready codebases.

**The Prompt Engineering Strategy:**
Initial attempts with Gemini 2.5 Pro using simple prompts proved insufficient, as the LLM failed to generate usable code in one shot, omitting crucial aspects like Server-Sent Events (SSE) and JSON-RPC, which are mandated by the MCP protocol. Laforge then adopted a strategy of feeding extensive context into a single, comprehensive prompt. This included:
1.  Specifying Micronaut 4.8 and Java 21.
2.  Providing the URL to the Micronaut documentation.
3.  Including the full MCP specification via its `llms-full.txt` URL.
4.  Attaching the complete LangChain4j MCP client source code (using `gitingest`), explicitly instructing Gemini to reuse these classes for the server implementation.
5.  Defining the goal: a simple MCP server to provide a fake weather forecast (`{"forecast": "sunny"}`).

**Iterative Development and Key Implementations:**
Even with the highly detailed prompt, the initial code generated by Gemini wasn't immediately functional because it didn't use Server-Sent Events. Laforge issued a follow-up prompt specifically requesting the Micronaut controller to utilize HTTP Server-Sent Events. This refinement led to a running server.

Gemini successfully generated key components:
*   An `SseBroadcaster` class responsible for handling Server-Sent Event communication, leveraging Reactor's `Publisher`, `Flux`, and `Sinks`, along with Micronaut's `JsonMapper` and SSE support.
*   A `PostController` which manages various JSON-RPC operations as defined by the MCP protocol. This controller includes logic to handle:
    *   `initialize`: Returning an `InitializeResult` with `ServerCapabilities`.
    *   `notifications/initialized`: Acknowledging client notifications.
    *   `tools/list`: Providing a list of available tools, specifically defining a `getWeatherForecast` tool.
    *   `tools/call`: Executing the `getWeatherForecast` tool and returning predefined fake weather data (`{"forecast": "sunny"}`).
    *   `ping`: Responding to ping requests.
    *   Error handling for unsupported methods or invalid parameters.

**Transition to AI-Assisted Testing:**
After the initial "vibe coding" phase, Laforge moved to a more conventional AI-assisted development approach using Gemini Code Assist within IntelliJ IDEA. He tasked it with transforming his existing MCP client code into a proper JUnit test suite. Gemini successfully created a Micronaut-aware unit test that launched an embedded server. The tests verify:
*   `testListTools`: Ensures the server correctly lists available tools, specifically confirming the presence of the `getWeatherForecast` tool.
*   `testWeatherRequest`: Simulates a client requesting the weather in "Paris" and asserts that the server responds with the expected fake "sunny" forecast.

**Conclusion and Future Outlook:**
The article concludes by making the developed code available in a GitHub repository (`glaforge/langchain4j-micronaut-mcp`), noting that it uses the latest LangChain4j dependencies (1.0.0-rc1). While the implementation serves as a strong starting point, it doesn't encompass all aspects of the MCP specification (e.g., prompts, resources, sampling).

Laforge expresses a desire for Micronaut to eventually offer a dedicated MCP server module, complete with annotations, to simplify and streamline the implementation of MCP servers. He also suggests deploying such Micronaut MCP applications to Google Cloud Run, referencing a previous article on deploying Micronaut apps to the platform.