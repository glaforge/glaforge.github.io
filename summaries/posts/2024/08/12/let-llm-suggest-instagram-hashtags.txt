The article delves into the powerful capabilities of Large Language Models (LLMs) for entity and data extraction, moving beyond their typical role as chatbots. It uses the practical example of generating Instagram hashtags to illustrate both the potential and a critical limitation of LLMs when performing structured output tasks.

**Initial Exploration: Creative Hashtag Generation (Free-Form)**
The author begins by showcasing how LLMs, specifically Google Gemini (a multimodal model accepting text and images), can creatively suggest Instagram hashtags for a given picture. Using an image from the port of Heraklion in Crete, Gemini successfully identified location-specific tags (e.g., `#heraklion`, `#crete`, `#greece`), general travel hashtags (e.g., `#travelgram`, `#instatravel`), and more niche tags (e.g., `#cretephotography`). This was achieved by prompting the LLM with a system message establishing an "Instagram influencer expert" persona and a user message containing the image and a request for hashtags. The initial, free-form response from Gemini 1.5 Flash was extensive, categorized (General, Specific, Trending, Engagement), and included helpful tips for hashtag selection, demonstrating high creativity and relevant domain knowledge.

**The Challenge: Reduced Creativity with Structured Output**
The author then attempts to integrate this functionality into an application that requires structured output, specifically a JSON array of hashtags. By configuring the `VertexAiGeminiChatModel` with `responseMimeType("application/json")` and updating the system prompt to explicitly request a JSON array example, the LLM successfully adhered to the format. However, a significant issue emerged: the JSON output contained noticeably fewer and less creative hashtags compared to the original free-form response.

**Key Argument and Supporting Research:**
This observation leads to the article's central argument: LLMs tend to exhibit reduced creativity when constrained to generate structured outputs. The author corroborates this intuition by referencing a recent arXiv paper, "Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models." This research empirically confirms that structured generation, while valuable for specific extraction tasks, can lead to a "significant decline in LLMs' reasoning abilities" and that stricter format constraints generally result in greater performance degradation in reasoning tasks.

**The Solution: A Two-Step Entity Extraction Approach**
To circumvent this limitation, the article proposes a "two-step approach" that leverages LLMs' inherent strength in entity extraction.
1.  **Creative Generation (First Call):** The first step involves an LLM call configured for free-form text generation, as in the initial demonstration. This allows the model to produce its most creative and comprehensive response without being limited by output format constraints.
2.  **Structured Extraction (Second Call):** The second step uses a separate LLM call, specifically tasked with extracting the desired entities (hashtags) from the raw, creative text generated in the first step. For this, the article demonstrates using Gemini 1.5 Pro with `responseSchema(SchemaHelper.fromClass(String[].class))`, allowing precise control over the JSON output schema. The system message for this second call clearly defines its role as extracting hashtags from a given text into a JSON array. This method successfully extracted all the detailed and creative hashtags from the first response into the required JSON format.

**Conclusion and Trade-offs:**
The article concludes that while LLMs may struggle with creativity under strict output constraints, a two-step approach — separating creative content generation from structured data extraction — provides an effective workaround. This strategy allows developers to harness the full creative potential of LLMs while still obtaining structured, machine-readable data.

However, the author also highlights the trade-offs associated with this solution. Making two separate LLM calls inherently increases both the **cost** (due to a higher number of tokens generated across two requests) and **latency** (as the second call must wait for the first to complete). The article emphasizes that the choice between a single constrained call and the two-step approach depends on the specific use case and the required balance between output quality, cost, and response time. The flexibility to choose between these methods is presented as a valuable capability in LLM application design.