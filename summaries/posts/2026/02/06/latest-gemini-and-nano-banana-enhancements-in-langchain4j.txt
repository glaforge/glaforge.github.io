The recent release of LangChain4j version 1.11.0 introduces a comprehensive suite of notable enhancements specifically designed to bolster support for the Google Gemini model family. These improvements significantly expand the capabilities for developers looking to integrate advanced AI features, including cutting-edge image generation, various forms of external data grounding, multimodal interactions, and finer control over model reasoning.

One of the most significant additions is the support for **new Image Generation Models**, specifically the Gemini 2.5 and 3.0 Preview models, collectively referred to as ":banana: Nano Banana." A new `GoogleAiGeminiImageModel` class is introduced, enabling both **text-to-image generation** and **image editing** (including optional mask support) using these advanced models.
*   **`gemini-2.5-flash-image` (Nano Banana)** is optimized for speed.
*   **`gemini-3-pro-image-preview` (Nano Banana Pro)** offers high-fidelity image generation, supporting resolutions up to 4K.
Developers can configure various parameters like `aspectRatio` (e.g., `16:9`, `1:1`) and `imageSize` (`1K`, `2K`, `4K`) for precise control over the generated output.

A powerful capability introduced with Nano Banana Pro is **Image Generation with Google Search Grounding**. By setting `useGoogleSearchGrounding(true)`, the model can actively search for image references on the web or retrieve the latest information about a topic to inform its image generation. For instance, a prompt asking for a "kawaii illustration of the current weather forecast for Paris" would cause the model to perform a live Google Search for Paris's weather and incorporate that real-time data into the generated image.

Beyond image generation, LangChain4j 1.11.0 extends grounding capabilities to text models. **Google Maps Grounding** is now available for Gemini 2.5 models (though not yet 3.0). Enabling `allowGoogleMaps(true)` allows the chat model to access real-world location data, including place IDs, addresses, and user reviews from Google Maps. This enables sophisticated queries like "Find the best restaurant near the Eiffel Tower," with the model providing detailed, star-rated recommendations directly sourced from Maps, rather than its pre-trained knowledge. An optional `retrieveGoogleMapsWidgetToken(true)` flag is also available for UI integration.

Similarly, **Google Search Grounding** is also enabled for standard text generation. By setting `allowGoogleSearch(true)` for models like `gemini-3-flash-preview`, responses are grounded in up-to-date web information. This ensures that answers to queries like "What are the latest models from OpenAI, Anthropic, and Google?" are current and factual, leveraging real-time search results.

The **URL Context Tool** further enhances context awareness by allowing models to access and use information directly from specific URLs provided within the prompt. With `allowUrlContext(true)`, developers no longer need to manually fetch or scrape content from a URL before making an LLM call; the model itself can process and reason over the content of a given webpage. For example, a prompt could ask the model to analyze an article archive at a specific URL and answer questions based on its content.

The release also introduces support for **Multimodal Agents** by allowing `AiServices` to return generated images directly. This enables the creation of agents that can produce visual content as part of their response. Using an `AiService` interface, developers can define methods that take a text description and return an `ImageContent` object, with the underlying Gemini image model (e.g., `gemini-3-pro-image-preview`) handling the generation. This highlights Nano Banana's capability as a chat model with both text and image response modalities.

For more advanced control, **Gemini 3.0 Thinking Configuration** is introduced, allowing developers to configure the "thinking" process (Chain-of-Thought) for Gemini 3.0 models. Parameters like `sendThinking(true)` and `returnThinking(true)` enable the model to send and return its internal thought process. The `thinkingConfig` can specify a `thinkingLevel` from `MINIMAL` to `HIGH`, controlling the depth of reasoning the model applies, useful for complex logic puzzles or intricate problem-solving.

Finally, the update brings **Enhanced Metadata & Token Usage** reporting. Responses from both chat and image models now include richer metadata, such as detailed token usage for input and output, and crucial grounding source information. Developers can cast the response metadata to `GoogleAiGeminiChatResponseMetadata` to access details about which web pages or map locations were used as grounding sources, providing valuable insights for debugging, optimization, and understanding model behavior.

In conclusion, LangChain4j 1.11.0 significantly deepens its integration with the Gemini model family, offering powerful new features like advanced image generation and editing with Nano Banana, real-time grounding through Google Search and Google Maps, direct URL content processing, and the ability to build sophisticated multimodal agents. The added control over Gemini 3.0's reasoning and enhanced metadata reporting provide developers with robust tools to build more intelligent, context-aware, and controllable AI applications.